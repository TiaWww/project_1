![](./images/data_cleaning.png)
# <center> Анализ резюме из HeadHunter </center>
## Оглавление
1. [Описание проекта](#описание-проекта)
2. [Описание данных](#описание-данных)
3. [Зависимости](#используемые-зависимости)
4. [Использование проекта](#использование)
5. [Авторы](#авторы)
6. [Выводы](#выводы)

## Описание проекта

Для отображения всех элементов проекта (включая графики), перейдите по [ссылке](https://nbviewer.org/github/TiaWww/project_1/blob/b410a294d1ba117f68bd9d96643bda1f781a1ad8/Project-1.ipynb)

Основные этапы работы над проектом:

1. Базовый анализ структуры данных

> **Базовый анализ** – это процесс структурирования и оценки данных, собранных в рамках исследования или анализа. Он является первым шагом в обработке данных, предшествующим более глубокому и подробному исследованию.

2. Преобразование данных

> **Преобразование данных** - это процесс преобразования данных в формат, пригодный для анализа. Цель преобразования данных - подготовить данные для дальнейшего анализа, сделав их непротиворечивыми, полными и в формате, который может быть легко использован аналитическими инструментами.

3. Разведывательный анализ

> **Разведочный анализ данных (EDA)** — анализ основных свойств данных, нахождение в них общих закономерностей, распределений и аномалий, построение начальных моделей, зачастую с использованием инструментов визуализации.

4. Очистка данных:
 * Работа с пропущенными значениями.
 * Очистка данных от пропусков.
 * Удаление признаков и записей, которые не несут полезной информации.

 > **Очистка данных (data cleaning)** – это процесс обнаружения и удаления (или исправления) поврежденных, ложных или неинформативных записей таблицы или целой базы данных. Процесс состоит из двух этапов: поиск и ликвидация (или редактирование).


**Данный проект** направлен на демонстрацию базового анализа структуры данных, их преобразование, разведывательный анализ с использованием инструментов визуализации и применения методов очистки данных на каждом из ее этапов на примере датасета c резюме из HeadHunter.

**О структуре проекта:**
* [data](./data) - папка с исходными табличными данными

* [Project-1.ipynb](./Project-1.ipynb) - jupyter-ноутбук, содержащий основной код проекта, в котором демонстрируются методы и подходы решения задач очистки данных


## Описание данных

В этом проекте используется датасет с резюме из HeadHunter

 Компания HeadHunter хочет построить модель, которая бы автоматически определяла примерный уровень заработной платы, подходящей пользователю, исходя из информации, которую он указал о себе. Прежде чем построить модель, данные необходимо преобразовать, исследовать и очистить.

Исходный датасет представляет собой набор данных из таблицы с информацией о параметрах, которые указыват о себе соискатели (пол, возраст, предыдущее место работы, опыт работы, город и т.д.)

Файл с данными можно найти [здесь](https://drive.google.com/file/d/1Kb78mAWYKcYlellTGhIjPI-bCcKbGuTn/view?pli=1).

## Используемые зависимости
* Python (3.9):
    * [numpy (1.24.2)](https://numpy.org)
    * [pandas (2.0.0)](https://pandas.pydata.org)
    * [matplotlib (3.7.2)](https://matplotlib.org)
    * [seaborn (0.12.2)](https://seaborn.pydata.org)
    * [plotly (5.15.0)](https://plotly.com/python/)


## Использование
Вся информация о работе представлена в jupyter-ноутбуке Project-1.ipynb.

## Авторы

* [Татьяна Волкова](https://vk.com/commonname)

## Выводы

В ходе работы надпроектом были преобразованы данные о городе проживания соискателей, опыт их работы в месяцах,выделен признак уровня образования, отдельно выделены признаки готовности к переезду и командировкам, выделены данные о предпочтительных видах занятости и желаемых графиках. Данные о желаемой заработной плате приведены в единую форму - сумма, указанная в рублях.

В ходе разведывательного анализа данных установлены взаимосвязи между желаемой ЗП и городом проживания; опытом работы, возрастом и зарплатными ожиданиями; опытом работы и желаемой ЗП, уровнем образования и желаемой ЗП; проанализировано распределение признаков "Возраст", "Желаемая ЗП", "Опыт работы"

На этапе очистки данных было произведено удаление дубликатов, заполнение пропусков в данных, а также очистка от выбросов.
